{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False, False, False, False, False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "gt_mask = torch.ones((2, 4))\n",
    "gt_mask[0, 3] = 0\n",
    "gt_mask = gt_mask.unsqueeze(-1)\n",
    "gt_mask = gt_mask.expand(-1, -1, 8).bool()\n",
    "print(gt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[10.,  1.,  2.,  1.,  1.,  1.,  1., 10.],\n",
      "         [ 3.,  4.,  5.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [ 7.,  8.,  9.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [ 1.,  4.,  7.,  1.,  1.,  1.,  1.,  1.]],\n",
      "\n",
      "        [[10.,  1.,  2.,  1.,  1.,  1.,  1., 10.],\n",
      "         [ 0.,  4.,  5.,  1.,  1.,  1.,  1.,  1.],\n",
      "         [ 7.,  0.,  0.,  1.,  1.,  9.,  1.,  1.],\n",
      "         [ 1.,  4.,  7.,  1.,  1.,  1.,  1.,  1.]]])\n",
      "tensor([[[10., 10.,  2.],\n",
      "         [ 5.,  4.,  3.],\n",
      "         [ 9.,  8.,  7.],\n",
      "         [ 7.,  4.,  1.]],\n",
      "\n",
      "        [[10., 10.,  2.],\n",
      "         [ 5.,  4.,  1.],\n",
      "         [ 9.,  7.,  1.],\n",
      "         [ 7.,  4.,  1.]]])\n",
      "tensor([[[7, 0, 2],\n",
      "         [2, 1, 0],\n",
      "         [2, 1, 0],\n",
      "         [2, 1, 7]],\n",
      "\n",
      "        [[7, 0, 2],\n",
      "         [2, 1, 7],\n",
      "         [5, 0, 6],\n",
      "         [2, 1, 7]]])\n"
     ]
    }
   ],
   "source": [
    "align_metric=torch.tensor([[[10,1,2,1,1,1,1,10],\n",
    "                            [3,4,5,1,1,1,1,1],\n",
    "                            [7,8,9,1,1,1,1,1],\n",
    "                            [1,4,7,1,1,1,1,1]],\n",
    "\n",
    "                            [[10,1,2,1,1,1,1,10],\n",
    "                            [0,4,5,1,1,1,1,1],\n",
    "                            [7,0,0,1,1,9,1,1],\n",
    "                            [1,4,7,1,1,1,1,1]]],dtype=torch.float32)\n",
    "\n",
    "topk = torch.topk(align_metric, k=3, dim=-1, largest=True)\n",
    "print(align_metric)\n",
    "print(topk[0])\n",
    "print(topk[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True,  True],\n",
      "         [ True,  True,  True],\n",
      "         [ True,  True,  True],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [ True,  True,  True],\n",
      "         [ True,  True,  True],\n",
      "         [ True,  True,  True]]])\n",
      "tensor([[[7, 0, 2],\n",
      "         [2, 1, 0],\n",
      "         [2, 1, 0],\n",
      "         [0, 0, 0]],\n",
      "\n",
      "        [[7, 0, 2],\n",
      "         [2, 1, 7],\n",
      "         [5, 0, 6],\n",
      "         [2, 1, 7]]])\n"
     ]
    }
   ],
   "source": [
    "topk_mask = torch.ones((2, 4, 1))\n",
    "topk_mask[0, 3] = 0\n",
    "topk_mask = topk_mask.expand(-1, -1, 3).bool()\n",
    "print(topk_mask)\n",
    "\n",
    "topk_idxs = topk[1]\n",
    "topk_idxs.masked_fill_(~topk_mask, 0)\n",
    "print(topk_idxs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 0, 1, 0, 0, 0, 0, 1],\n",
      "         [1, 1, 1, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[1, 0, 1, 0, 0, 0, 0, 1],\n",
      "         [0, 1, 1, 0, 0, 0, 0, 1],\n",
      "         [1, 0, 0, 0, 0, 1, 1, 0],\n",
      "         [0, 1, 1, 0, 0, 0, 0, 1]]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "count_tensor = torch.zeros(align_metric.shape, dtype=torch.int8, device=topk_idxs.device)\n",
    "ones = torch.ones_like(topk_idxs[:, :, :1], dtype=torch.int8, device=topk_idxs.device)\n",
    "for k in range(3):\n",
    "    count_tensor.scatter_add_(-1, topk_idxs[:, :, k:k + 1], ones)\n",
    "count_tensor.masked_fill_(count_tensor > 1, 0)\n",
    "print(count_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 1, 0, 0, 0, 0, 1],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[1, 0, 1, 0, 0, 0, 0, 1],\n",
       "         [0, 1, 1, 0, 0, 0, 0, 1],\n",
       "         [1, 0, 0, 0, 0, 1, 1, 0],\n",
       "         [0, 1, 1, 0, 0, 0, 0, 1]]], dtype=torch.int8)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_pos = gt_mask * count_tensor\n",
    "mask_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 2, 3, 0, 0, 0, 0, 1],\n",
       "        [2, 2, 3, 0, 0, 1, 1, 3]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fg_mask = mask_pos.sum(-2)\n",
    "fg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False]],\n",
       "\n",
       "        [[ True,  True,  True, False, False, False, False,  True],\n",
       "         [ True,  True,  True, False, False, False, False,  True],\n",
       "         [ True,  True,  True, False, False, False, False,  True],\n",
       "         [ True,  True,  True, False, False, False, False,  True]]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_multi_gts = (fg_mask.unsqueeze(1) > 1).expand(-1, 4, -1)\n",
    "mask_multi_gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0148, 0.5420, 0.0744, 0.3701, 0.5614, 0.0080, 0.3767, 0.3373],\n",
       "         [0.6202, 0.2900, 0.7835, 0.6297, 0.6616, 0.9272, 0.0851, 0.2147],\n",
       "         [0.9434, 0.7546, 0.3585, 0.0242, 0.7593, 0.7786, 0.5190, 0.8372],\n",
       "         [0.3798, 0.2589, 0.8041, 0.1986, 0.2092, 0.5301, 0.3496, 0.0403]],\n",
       "\n",
       "        [[0.1429, 0.8005, 0.4486, 0.1503, 0.9951, 0.3046, 0.9802, 0.1799],\n",
       "         [0.0230, 0.8320, 0.6076, 0.5870, 0.9751, 0.8765, 0.4471, 0.6931],\n",
       "         [0.4278, 0.2421, 0.9587, 0.6602, 0.2421, 0.5515, 0.8688, 0.9818],\n",
       "         [0.1962, 0.5363, 0.4034, 0.0046, 0.3641, 0.0843, 0.4413, 0.8637]]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlaps = torch.rand((2, 4, 8))\n",
    "overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 3, 1, 2, 1, 2, 2],\n",
       "        [2, 1, 2, 2, 0, 1, 0, 2]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_overlaps_idx = overlaps.argmax(1)  # (b, h*w)\n",
    "max_overlaps_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0]]], dtype=torch.int8)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_max_overlaps = torch.zeros(mask_pos.shape, dtype=mask_pos.dtype, device=mask_pos.device)\n",
    "is_max_overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2, 2, 3, 1, 2, 1, 2, 2]],\n",
       "\n",
       "        [[2, 1, 2, 2, 0, 1, 0, 2]]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_overlaps_idx.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 1, 0, 0],\n",
       "         [1, 1, 0, 0, 1, 0, 1, 1],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 1, 0, 1, 0],\n",
       "         [0, 1, 0, 0, 0, 1, 0, 0],\n",
       "         [1, 0, 1, 1, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0]]], dtype=torch.int8)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_max_overlaps.scatter_(1, max_overlaps_idx.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False]],\n",
       "\n",
       "        [[ True,  True,  True, False, False, False, False,  True],\n",
       "         [ True,  True,  True, False, False, False, False,  True],\n",
       "         [ True,  True,  True, False, False, False, False,  True],\n",
       "         [ True,  True,  True, False, False, False, False,  True]]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_multi_gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 1, 0, 0, 0, 0, 1],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[1, 0, 1, 0, 0, 0, 0, 1],\n",
       "         [0, 1, 1, 0, 0, 0, 0, 1],\n",
       "         [1, 0, 0, 0, 0, 1, 1, 0],\n",
       "         [0, 1, 1, 0, 0, 0, 0, 1]]], dtype=torch.int8)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 0., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_pos = torch.where(mask_multi_gts, is_max_overlaps, mask_pos).float()\n",
    "mask_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 0., 0., 0., 0., 1.],\n",
       "        [1., 1., 1., 0., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fg_mask = mask_pos.sum(-2)\n",
    "fg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 3, 0, 0, 0, 0, 0],\n",
       "        [2, 1, 2, 0, 0, 2, 2, 2]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_gt_idx = mask_pos.argmax(-2)\n",
    "target_gt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9],\n",
       "         [ 5],\n",
       "         [ 1],\n",
       "         [ 0]],\n",
       "\n",
       "        [[ 7],\n",
       "         [10],\n",
       "         [ 0],\n",
       "         [11]]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_labels = torch.randint(0, 16, (2, 4, 1))\n",
    "gt_labels[0, 3, 0] = 0\n",
    "gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigned target labels, (b, 1)\n",
    "batch_ind = torch.arange(end=2, dtype=torch.int64, device=gt_labels.device)[..., None]\n",
    "batch_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 3, 0, 0, 0, 0, 0],\n",
       "        [6, 5, 6, 4, 4, 6, 6, 6]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_gt_idx = target_gt_idx + batch_ind * 4\n",
    "target_gt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9,  5,  1,  0,  7, 10,  0, 11])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_labels.long().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  0,  9,  9,  9,  9,  9],\n",
       "        [ 0, 10,  0,  7,  7,  0,  0,  0]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_labels = gt_labels.long().flatten()[target_gt_idx]\n",
    "target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 30, 289, 294,  41],\n",
       "         [216, 170,  15,  90],\n",
       "         [ 59,  10,  31, 115],\n",
       "         [257, 271, 317, 271]],\n",
       "\n",
       "        [[101, 261,  72,   6],\n",
       "         [ 97,  71,  57,  78],\n",
       "         [224,  44,  76, 189],\n",
       "         [304, 215, 244,  58]]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_bboxes = torch.randint(0, 320, (2, 4, 4))\n",
    "gt_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 59,  10,  31, 115],\n",
       "         [ 59,  10,  31, 115],\n",
       "         [257, 271, 317, 271],\n",
       "         [ 30, 289, 294,  41],\n",
       "         [ 30, 289, 294,  41],\n",
       "         [ 30, 289, 294,  41],\n",
       "         [ 30, 289, 294,  41],\n",
       "         [ 30, 289, 294,  41]],\n",
       "\n",
       "        [[224,  44,  76, 189],\n",
       "         [ 97,  71,  57,  78],\n",
       "         [224,  44,  76, 189],\n",
       "         [101, 261,  72,   6],\n",
       "         [101, 261,  72,   6],\n",
       "         [224,  44,  76, 189],\n",
       "         [224,  44,  76, 189],\n",
       "         [224,  44,  76, 189]]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_bboxes = gt_bboxes.view(-1, 4)[target_gt_idx]\n",
    "target_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  0,  9,  9,  9,  9,  9],\n",
       "        [ 0, 10,  0,  7,  7,  0,  0,  0]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_labels.clamp_(0)\n",
    "target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10x faster than F.one_hot()\n",
    "target_scores = torch.zeros((target_labels.shape[0], target_labels.shape[1], 16),\n",
    "                            dtype=torch.int64,\n",
    "                            device=target_labels.device)  # (b, h*w, 80)\n",
    "\n",
    "target_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_scores.scatter_(2, target_labels.unsqueeze(-1), 1)\n",
    "target_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fg_scores_mask = fg_mask[:, :, None].repeat(1, 1, 16)\n",
    "fg_scores_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_scores = torch.where(fg_scores_mask > 0, target_scores, 0)\n",
    "target_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 16])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  0.,  0.,  0.,  0.,  0.,  0., 10.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 7.,  8.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  7.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  4.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 7.,  0.,  0.,  0.,  0.,  9.,  1.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize\n",
    "align_metric *= mask_pos\n",
    "align_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10.],\n",
       "         [ 0.],\n",
       "         [ 8.],\n",
       "         [ 7.]],\n",
       "\n",
       "        [[ 0.],\n",
       "         [ 4.],\n",
       "         [ 9.],\n",
       "         [ 0.]]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_align_metrics = align_metric.amax(axis=-1, keepdim=True)\n",
    "pos_align_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3373],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.9434, 0.7546, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.8041, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.8320, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4278, 0.0000, 0.9587, 0.0000, 0.0000, 0.5515, 0.8688, 0.9818],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlaps * mask_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3373],\n",
       "         [0.0000],\n",
       "         [0.9434],\n",
       "         [0.8041]],\n",
       "\n",
       "        [[0.0000],\n",
       "         [0.8320],\n",
       "         [0.9818],\n",
       "         [0.0000]]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_overlaps = (overlaps * mask_pos).amax(axis=-1, keepdim=True)\n",
    "pos_overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3373],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8255, 0.9434, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.8041, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.8320, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7636, 0.0000, 0.0000, 0.0000, 0.0000, 0.9818, 0.1091, 0.1091],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_metric * pos_overlaps / (pos_align_metrics + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8255],\n",
       "         [0.9434],\n",
       "         [0.8041],\n",
       "         [0.0000],\n",
       "         [0.0000],\n",
       "         [0.0000],\n",
       "         [0.0000],\n",
       "         [0.3373]],\n",
       "\n",
       "        [[0.7636],\n",
       "         [0.8320],\n",
       "         [0.0000],\n",
       "         [0.0000],\n",
       "         [0.0000],\n",
       "         [0.9818],\n",
       "         [0.1091],\n",
       "         [0.1091]]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_align_metric = (align_metric * pos_overlaps / (pos_align_metrics + 1e-9)).amax(-2).unsqueeze(-1)\n",
    "norm_align_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_scores = target_scores * norm_align_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.8255, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.9434, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8041, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.3373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 4]), torch.Size([2, 8, 16]), torch.Size([2, 8]))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_bboxes.size(), target_scores.size(), fg_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def autopad(k, p=None, d=1):  # kernel, padding, dilation\n",
    "    \"\"\"Pad to 'same' shape outputs.\"\"\"\n",
    "    if d > 1:\n",
    "        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size\n",
    "    if p is None:\n",
    "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n",
    "    return p\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    \"\"\"Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation).\"\"\"\n",
    "    default_act = nn.SiLU()  # default activation\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n",
    "        \"\"\"Initialize Conv layer with given arguments including activation.\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c2)\n",
    "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "    def forward_fuse(self, x):\n",
    "        \"\"\"Perform transposed convolution of 2D data.\"\"\"\n",
    "        return self.act(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8., 16., 32.])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "feats = [torch.rand((batch_size, 256, 80, 80)), \n",
    "         torch.rand((batch_size, 512, 40, 40)), \n",
    "         torch.rand((batch_size, 512, 20, 20))]\n",
    "\n",
    "s = 640\n",
    "strides = torch.tensor([s / x.shape[-2] for x in feats])\n",
    "strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 256\n",
      "torch.Size([2, 64, 80, 80]) torch.Size([2, 16, 80, 80])\n",
      "torch.Size([2, 80, 80, 80])\n",
      "torch.Size([2, 64, 40, 40]) torch.Size([2, 16, 40, 40])\n",
      "torch.Size([2, 80, 40, 40])\n",
      "torch.Size([2, 64, 20, 20]) torch.Size([2, 16, 20, 20])\n",
      "torch.Size([2, 80, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def autopad(k, p=None, d=1):  # kernel, padding, dilation\n",
    "    \"\"\"Pad to 'same' shape outputs.\"\"\"\n",
    "    if d > 1:\n",
    "        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size\n",
    "    if p is None:\n",
    "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n",
    "    return p\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    \"\"\"Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation).\"\"\"\n",
    "    default_act = nn.SiLU()  # default activation\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n",
    "        \"\"\"Initialize Conv layer with given arguments including activation.\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c2)\n",
    "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "    def forward_fuse(self, x):\n",
    "        \"\"\"Perform transposed convolution of 2D data.\"\"\"\n",
    "        return self.act(self.conv(x))\n",
    "    \n",
    "\n",
    "ch = (256, 512, 512)\n",
    "\n",
    "reg_max = 16 \n",
    "nc = 16\n",
    "\n",
    "c2, c3 = max((16, ch[0] // 4, reg_max * 4)), max(ch[0], min(nc, 100))\n",
    "\n",
    "print(c2, c3)\n",
    "\n",
    "cv2 = nn.ModuleList(\n",
    "    nn.Sequential(\n",
    "        Conv(x, c2, 3), \n",
    "        Conv(c2, c2, 3),\n",
    "        nn.Conv2d(c2, 4 * reg_max, 1)) for x in ch)\n",
    "\n",
    "cv3 = nn.ModuleList(\n",
    "    nn.Sequential(\n",
    "        Conv(x, c3, 3), \n",
    "        Conv(c3, c3, 3),\n",
    "        nn.Conv2d(c3, nc, 1)) for x in ch)\n",
    "\n",
    "\n",
    "for i in range(len(ch)):\n",
    "    print(cv2[i](feats[i]).size(),cv3[i](feats[i]).size())\n",
    "    print(torch.cat((cv2[i](feats[i]), cv3[i](feats[i])), 1).size())\n",
    "    feats[i] = torch.cat((cv2[i](feats[i]), cv3[i](feats[i])), 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "no = nc + reg_max * 4 \n",
    "no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 64, 8400]), torch.Size([2, 16, 8400]))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], no, -1) for xi in feats], 2).split((reg_max * 4, nc), 1)\n",
    "pred_distri.size(), pred_scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8400, 64]), torch.Size([2, 8400, 16]))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_distri = pred_distri.permute(0, 2, 1).contiguous()\n",
    "pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n",
    "pred_distri.size(), pred_scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 64]), torch.Size([2, 8, 16]))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_distri = torch.rand((2, 8, 16 * 4))\n",
    "pred_scores = torch.rand((2, 8, 16))\n",
    "pred_distri.size(), pred_scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(245.5353)\n",
      "tensor(245.5353)\n"
     ]
    }
   ],
   "source": [
    "bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "print(bce(pred_scores, target_scores).sum())\n",
    "print(-(target_scores * pred_scores.sigmoid().log() + (1 - target_scores) * (1 - pred_scores.sigmoid()).log()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8400, 2])\n",
      "torch.Size([8400, 1])\n",
      "torch.Size([8, 2])\n",
      "torch.Size([8, 1])\n"
     ]
    }
   ],
   "source": [
    "def make_anchors(feats, strides, grid_cell_offset=0.5):\n",
    "    \"\"\"Generate anchors from features.\"\"\"\n",
    "    anchor_points, stride_tensor = [], []\n",
    "    assert feats is not None\n",
    "    dtype, device = feats[0].dtype, feats[0].device\n",
    "    for i, stride in enumerate(strides):\n",
    "        _, _, h, w = feats[i].shape\n",
    "        sx = torch.arange(end=w, device=device, dtype=dtype) + grid_cell_offset  # shift x\n",
    "        sy = torch.arange(end=h, device=device, dtype=dtype) + grid_cell_offset  # shift y\n",
    "        sy, sx = torch.meshgrid(sy, sx, indexing='ij') \n",
    "        anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
    "        stride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device))\n",
    "    return torch.cat(anchor_points), torch.cat(stride_tensor)\n",
    "\n",
    "anchor_points, stride_tensor = make_anchors(feats, strides, 0.5)\n",
    "\n",
    "print(anchor_points.size())\n",
    "print(stride_tensor.size())\n",
    "\n",
    "anchor_points = torch.rand((8, 2))\n",
    "stride_tensor = torch.randint(8, 32, (8, 1)).float()\n",
    "\n",
    "print(anchor_points.size())\n",
    "print(stride_tensor.size())\n",
    "\n",
    "def bbox2dist(anchor_points, bbox, reg_max):\n",
    "    \"\"\"Transform bbox(xyxy) to dist(ltrb).\"\"\"\n",
    "    x1y1, x2y2 = bbox.chunk(2, -1)\n",
    "    return torch.cat((anchor_points - x1y1, x2y2 - anchor_points), -1).clamp_(0, reg_max - 0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 59,  10,  31, 115])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_bboxes[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stride_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9997, 0.5832])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_points[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.5556,  1.1111,  3.4444, 12.7778])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_bboxes = target_bboxes.float()\n",
    "target_bboxes /= stride_tensor\n",
    "target_bboxes[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  2.4448, 12.1946])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ltrb = bbox2dist(anchor_points, target_bboxes, 16 - 1)\n",
    "\n",
    "target_ltrb[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 64]), torch.Size([2, 8, 4]))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_distri.size(), target_ltrb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 64]), torch.Size([10, 4]))"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_distri[fg_mask.bool()].size(), target_ltrb[fg_mask.bool()].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 16])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_distri[fg_mask.bool()].view(-1, 15 + 1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  2, 12])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ltrb[fg_mask.bool()].long()[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  1,  3, 13])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(target_ltrb[fg_mask.bool()].long() + 1)[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 0.5552, 0.8054])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((target_ltrb[fg_mask.bool()].long() + 1) - target_ltrb[fg_mask.bool()])[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = target_ltrb[fg_mask.bool()].long()  # target left\n",
    "tr = tl + 1  # target right\n",
    "wl = tr - target_ltrb[fg_mask.bool()]  # weight left\n",
    "wr = 1 - wl  # weight right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  2.4448, 12.1946])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ltrb[fg_mask.bool()][0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  0,  2, 12]), tensor([1.0000, 1.0000, 0.5552, 0.8054]))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl[0, :], wl[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1,  1,  3, 13]), tensor([0.0000, 0.0000, 0.4448, 0.1946]))"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr[0, :], wr[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 16])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_distri[fg_mask.bool()].view(-1, 15 + 1).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7523],\n",
       "        [2.6878],\n",
       "        [2.9389],\n",
       "        [2.8822],\n",
       "        [2.8924],\n",
       "        [2.8146],\n",
       "        [2.8061],\n",
       "        [2.7107],\n",
       "        [2.8432],\n",
       "        [2.6574]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-(pred_distri[fg_mask.bool()].view(-1, 15 + 1).softmax(-1).log() * F.one_hot(tl.view(-1), num_classes=16)).sum(-1).view(tl.shape) * wl + \\\n",
    "(-(pred_distri[fg_mask.bool()].view(-1, 15 + 1).softmax(-1).log() * F.one_hot(tr.view(-1), num_classes=16)).sum(-1)).view(tl.shape) * wr).mean(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7523],\n",
       "        [2.6878],\n",
       "        [2.9389],\n",
       "        [2.8822],\n",
       "        [2.8924],\n",
       "        [2.8146],\n",
       "        [2.8061],\n",
       "        [2.7107],\n",
       "        [2.8432],\n",
       "        [2.6574]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(F.cross_entropy(pred_distri[fg_mask.bool()].view(-1, 15 + 1), tl.view(-1), reduction='none').view(tl.shape) * wl +\n",
    "F.cross_entropy(pred_distri[fg_mask.bool()].view(-1, 15 + 1), tr.view(-1), reduction='none').view(tl.shape) * wr).mean(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist2bbox(distance, anchor_points, xywh=True, dim=-1):\n",
    "    \"\"\"Transform distance(ltrb) to box(xywh or xyxy).\"\"\"\n",
    "    lt, rb = distance.chunk(2, dim)\n",
    "    x1y1 = anchor_points - lt\n",
    "    x2y2 = anchor_points + rb\n",
    "    if xywh:\n",
    "        c_xy = (x1y1 + x2y2) / 2\n",
    "        wh = x2y2 - x1y1\n",
    "        return torch.cat((c_xy, wh), dim)  # xywh bbox\n",
    "    return torch.cat((x1y1, x2y2), dim)  # xyxy bbox\n",
    "\n",
    "def bbox_decode(anchor_points, pred_dist):\n",
    "    \"\"\"Decode predicted object bounding box coordinates from anchor points and distribution.\"\"\"\n",
    "\n",
    "    b, a, c = pred_dist.shape  # batch, anchors, channels\n",
    "    pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul()\n",
    "  \n",
    "    return dist2bbox(pred_dist, anchor_points, xywh=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bbox_iou(box1, box2, xywh=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-7):\n",
    "\n",
    "    # Get the coordinates of bounding boxes\n",
    "    if xywh:  # transform from xywh to xyxy\n",
    "        (x1, y1, w1, h1), (x2, y2, w2, h2) = box1.chunk(4, -1), box2.chunk(4, -1)\n",
    "        w1_, h1_, w2_, h2_ = w1 / 2, h1 / 2, w2 / 2, h2 / 2\n",
    "        b1_x1, b1_x2, b1_y1, b1_y2 = x1 - w1_, x1 + w1_, y1 - h1_, y1 + h1_\n",
    "        b2_x1, b2_x2, b2_y1, b2_y2 = x2 - w2_, x2 + w2_, y2 - h2_, y2 + h2_\n",
    "    else:  # x1, y1, x2, y2 = box1\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n",
    "        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n",
    "        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n",
    "    # Intersection area\n",
    "    inter = (b1_x2.minimum(b2_x2) - b1_x1.maximum(b2_x1)).clamp_(0) * \\\n",
    "            (b1_y2.minimum(b2_y2) - b1_y1.maximum(b2_y1)).clamp_(0)\n",
    "\n",
    "    # Union Area\n",
    "    union = w1 * h1 + w2 * h2 - inter + eps\n",
    "\n",
    "    # IoU\n",
    "    iou = inter / union\n",
    "    if CIoU or DIoU or GIoU:\n",
    "        cw = b1_x2.maximum(b2_x2) - b1_x1.minimum(b2_x1)  # convex (smallest enclosing box) width\n",
    "        ch = b1_y2.maximum(b2_y2) - b1_y1.minimum(b2_y1)  # convex height\n",
    "        if CIoU or DIoU:  # Distance or Complete IoU \n",
    "            c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n",
    "            rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center dist ** 2\n",
    "            if CIoU: \n",
    "                v = (4 / math.pi ** 2) * (torch.atan(w2 / h2) - torch.atan(w1 / h1)).pow(2)\n",
    "                with torch.no_grad():\n",
    "                    alpha = v / (v - iou + (1 + eps))\n",
    "                return iou - (rho2 / c2 + v * alpha)  # CIoU\n",
    "            return iou - rho2 / c2  # DIoU\n",
    "        c_area = cw * ch + eps  # convex area\n",
    "        return iou - (c_area - union) / c_area \n",
    "    return iou  # IoU\n",
    "\n",
    "pd_bboxes = torch.rand((2, 8, 4))\n",
    "\n",
    "target_scores_sum = max(target_scores.sum(), 1)\n",
    "\n",
    "weight = target_scores.sum(-1)[fg_mask.bool()].unsqueeze(-1)\n",
    "iou = bbox_iou(pd_bboxes[fg_mask.bool()], target_bboxes[fg_mask.bool()], xywh=False, CIoU=True)\n",
    "loss_iou = ((1.0 - iou) * weight).sum() / target_scores_sum\n",
    "loss_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.yaml')\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autopad(k, p=None, d=1):  # kernel, padding, dilation\n",
    "    \"\"\"Pad to 'same' shape outputs.\"\"\"\n",
    "    if d > 1:\n",
    "        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size\n",
    "    if p is None:\n",
    "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n",
    "    return p\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    \"\"\"Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation).\"\"\"\n",
    "    default_act = nn.SiLU()  # default activation\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n",
    "        \"\"\"Initialize Conv layer with given arguments including activation.\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c2)\n",
    "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "    def forward_fuse(self, x):\n",
    "        \"\"\"Perform transposed convolution of 2D data.\"\"\"\n",
    "        return self.act(self.conv(x))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ShuffleChannel(nn.Module):\n",
    "\n",
    "    def __init__(self, g) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.g = g\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "\n",
    "        x = x.view()\n",
    "        x = torch.transpose()\n",
    "        x = x.view()\n",
    "\n",
    "        return x\n",
    "\n",
    "class ShuffleConv(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.GConv1 = nn.Sequential(\n",
    "            nn.Conv2d(),\n",
    "            nn.BatchNorm2d(),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        self.DWConv1 = nn.Sequential(\n",
    "            nn.Conv2d(),\n",
    "            nn.BatchNorm2d(),\n",
    "        )\n",
    "\n",
    "        self.GConv2 = nn.Sequential(\n",
    "            nn.Conv2d(),\n",
    "            nn.BatchNorm2d(),\n",
    "        )\n",
    "\n",
    "        self.shorcut = nn.Sequential(\n",
    "            nn.AvgPool2d()\n",
    "        )\n",
    "\n",
    "        self.shuffleChannel = ShuffleChannel(g)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.GConv1(x)\n",
    "        y = self.shuffleChannel(y)\n",
    "        y = self.DWConv1(y)\n",
    "        y = self.GConv2(y)\n",
    "        \n",
    "        short = self.shorcut(x)\n",
    "        torch.cat([y, short], dim=1)\n",
    "\n",
    "        return y\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFL(nn.Module):\n",
    "\n",
    "    def __init__(self, c1=16):\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, 1, 1, bias=False).requires_grad_(False)\n",
    "        x = torch.arange(c1, dtype=torch.float)\n",
    "        self.conv.weight.data[:] = nn.Parameter(x.view(1, c1, 1, 1))\n",
    "        self.c1 = c1\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, a = x.shape\n",
    "        return self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(b, 4, a)\n",
    "    \n",
    "class Detect(nn.Module):\n",
    "    \"\"\"YOLOv8 Detect head for detection models.\"\"\"\n",
    "    dynamic = False  # force grid reconstruction\n",
    "    export = False  # export mode\n",
    "    shape = None\n",
    "    anchors = torch.empty(0)  # init\n",
    "    strides = torch.empty(0)  # init\n",
    "\n",
    "    def __init__(self, nc=80, ch=()):  # detection layer\n",
    "        super().__init__()\n",
    "        self.nc = nc  # number of classes\n",
    "        self.nl = len(ch)  # number of detection layers\n",
    "        self.reg_max = 16  # DFL channels (ch[0] // 16 to scale 4/8/12/16/20 for n/s/m/l/x)\n",
    "        self.no = nc + self.reg_max * 4  # number of outputs per anchor\n",
    "        self.stride = torch.zeros(self.nl)  # strides computed during build\n",
    "        c2, c3 = max((16, ch[0] // 4, self.reg_max * 4)), max(ch[0], self.nc)  # channels\n",
    "        self.cv2 = nn.ModuleList(\n",
    "            nn.Sequential(Conv(x, c2, 3), Conv(c2, c2, 3), nn.Conv2d(c2, 4 * self.reg_max, 1)) for x in ch)\n",
    "        self.cv3 = nn.ModuleList(nn.Sequential(Conv(x, c3, 3), Conv(c3, c3, 3), nn.Conv2d(c3, self.nc, 1)) for x in ch)\n",
    "        self.dfl = DFL(self.reg_max) if self.reg_max > 1 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Concatenates and returns predicted bounding boxes and class probabilities.\"\"\"\n",
    "        shape = x[0].shape  # BCHW\n",
    "        for i in range(self.nl):\n",
    "            x[i] = torch.cat((self.cv2[i](x[i]), self.cv3[i](x[i])), 1)\n",
    "        if self.training:\n",
    "            return x\n",
    "        elif self.dynamic or self.shape != shape:\n",
    "            self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))\n",
    "            self.shape = shape\n",
    "\n",
    "        x_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)\n",
    "        if self.export and self.format in ('saved_model', 'pb', 'tflite', 'edgetpu', 'tfjs'):  # avoid TF FlexSplitV ops\n",
    "            box = x_cat[:, :self.reg_max * 4]\n",
    "            cls = x_cat[:, self.reg_max * 4:]\n",
    "        else:\n",
    "            box, cls = x_cat.split((self.reg_max * 4, self.nc), 1)\n",
    "        dbox = dist2bbox(self.dfl(box), self.anchors.unsqueeze(0), xywh=True, dim=1) * self.strides\n",
    "        y = torch.cat((dbox, cls.sigmoid()), 1)\n",
    "        return y if self.export else (y, x)\n",
    "\n",
    "    def bias_init(self):\n",
    "        \"\"\"Initialize Detect() biases, WARNING: requires stride availability.\"\"\"\n",
    "        m = self  # self.model[-1]  # Detect() module\n",
    "        # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1\n",
    "        # ncf = math.log(0.6 / (m.nc - 0.999999)) if cf is None else torch.log(cf / cf.sum())  # nominal class frequency\n",
    "        for a, b, s in zip(m.cv2, m.cv3, m.stride):  # from\n",
    "            a[-1].bias.data[:] = 1.0  # box\n",
    "            b[-1].bias.data[:m.nc] = math.log(5 / m.nc / (640 / s) ** 2)  # cls (.01 objects, 80 classes, 640 img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPPF(nn.Module):\n",
    "    \"\"\"Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, k=5):  # equivalent to SPP(k=(5, 9, 13))\n",
    "        super().__init__()\n",
    "        c_ = c1 // 2  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = Conv(c_ * 4, c2, 1, 1)\n",
    "        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through Ghost Convolution block.\"\"\"\n",
    "        x = self.cv1(x)\n",
    "        y1 = self.m(x)\n",
    "        y2 = self.m(y1)\n",
    "        return self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"Standard bottleneck.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):  # ch_in, ch_out, shortcut, groups, kernels, expand\n",
    "        super().__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, k[0], 1)\n",
    "        self.cv2 = Conv(c_, c2, k[1], 1, g=g)\n",
    "        self.add = shortcut and c1 == c2\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"'forward()' applies the YOLOv5 FPN to input data.\"\"\"\n",
    "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
    "\n",
    "class C2f(nn.Module):\n",
    "    \"\"\"CSP Bottleneck with 2 convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n",
    "        super().__init__()\n",
    "        self.c = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n",
    "        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)\n",
    "        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through C2f layer.\"\"\"\n",
    "        y = list(self.cv1(x).chunk(2, 1))\n",
    "        y.extend(m(y[-1]) for m in self.m)\n",
    "        return self.cv2(torch.cat(y, 1))\n",
    "\n",
    "    def forward_split(self, x):\n",
    "        \"\"\"Forward pass using split() instead of chunk().\"\"\"\n",
    "        y = list(self.cv1(x).split((self.c, self.c), 1))\n",
    "        y.extend(m(y[-1]) for m in self.m)\n",
    "        return self.cv2(torch.cat(y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def preprocess(self, targets, batch_size, scale_tensor):\n",
    "    \"\"\"Preprocesses the target counts and matches with the input batch size to output a tensor.\"\"\"\n",
    "    if targets.shape[0] == 0:\n",
    "        out = torch.zeros(batch_size, 0, 5, device=self.device)\n",
    "    else:\n",
    "        i = targets[:, 0]  # image index\n",
    "        _, counts = i.unique(return_counts=True)\n",
    "        counts = counts.to(dtype=torch.int32)\n",
    "        out = torch.zeros(batch_size, counts.max(), 5, device=self.device)\n",
    "        for j in range(batch_size):\n",
    "            matches = i == j\n",
    "            n = matches.sum()\n",
    "            if n:\n",
    "                out[j, :n] = targets[matches, 1:]\n",
    "        out[..., 1:5] = xywh2xyxy(out[..., 1:5].mul_(scale_tensor))\n",
    "    return out\n",
    "\n",
    "def bbox_decode(self, anchor_points, pred_dist):\n",
    "    \"\"\"Decode predicted object bounding box coordinates from anchor points and distribution.\"\"\"\n",
    "    if self.use_dfl:\n",
    "        b, a, c = pred_dist.shape  # batch, anchors, channels\n",
    "        pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n",
    "        # pred_dist = pred_dist.view(b, a, c // 4, 4).transpose(2,3).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n",
    "        # pred_dist = (pred_dist.view(b, a, c // 4, 4).softmax(2) * self.proj.type(pred_dist.dtype).view(1, 1, -1, 1)).sum(2)\n",
    "    return dist2bbox(pred_dist, anchor_points, xywh=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_ap(recall, precision):\n",
    "    \"\"\"\n",
    "    Compute the average precision (AP) given the recall and precision curves.\n",
    "\n",
    "    Arguments:\n",
    "        recall (list): The recall curve.\n",
    "        precision (list): The precision curve.\n",
    "\n",
    "    Returns:\n",
    "        (float): Average precision.\n",
    "        (np.ndarray): Precision envelope curve.\n",
    "        (np.ndarray): Modified recall curve with sentinel values added at the beginning and end.\n",
    "    \"\"\"\n",
    "\n",
    "    # Append sentinel values to beginning and end\n",
    "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
    "    mpre = np.concatenate(([1.0], precision, [0.0]))\n",
    "\n",
    "    # Compute the precision envelope\n",
    "    mpre = np.flip(np.maximum.accumulate(np.flip(mpre)))\n",
    "\n",
    "    # Integrate area under curve\n",
    "    method = 'interp'  # methods: 'continuous', 'interp'\n",
    "    if method == 'interp':\n",
    "        x = np.linspace(0, 1, 101)  # 101-point interp (COCO)\n",
    "        ap = np.trapz(np.interp(x, mrec, mpre), x)  # integrate\n",
    "    else:  # 'continuous'\n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0]  # points where x-axis (recall) changes\n",
    "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])  # area under curve\n",
    "\n",
    "    return ap, mpre, mrec\n",
    "\n",
    "\n",
    "def ap_per_class(tp,\n",
    "                 conf,\n",
    "                 pred_cls,\n",
    "                 target_cls,\n",
    "                 plot=False,\n",
    "                 on_plot=None,\n",
    "                 save_dir=Path(),\n",
    "                 names=(),\n",
    "                 eps=1e-16,\n",
    "                 prefix=''):\n",
    "    \"\"\"\n",
    "    Computes the average precision per class for object detection evaluation.\n",
    "\n",
    "    Args:\n",
    "        tp (np.ndarray): Binary array indicating whether the detection is correct (True) or not (False).\n",
    "        conf (np.ndarray): Array of confidence scores of the detections.\n",
    "        pred_cls (np.ndarray): Array of predicted classes of the detections.\n",
    "        target_cls (np.ndarray): Array of true classes of the detections.\n",
    "        plot (bool, optional): Whether to plot PR curves or not. Defaults to False.\n",
    "        on_plot (func, optional): A callback to pass plots path and data when they are rendered. Defaults to None.\n",
    "        save_dir (Path, optional): Directory to save the PR curves. Defaults to an empty path.\n",
    "        names (tuple, optional): Tuple of class names to plot PR curves. Defaults to an empty tuple.\n",
    "        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-16.\n",
    "        prefix (str, optional): A prefix string for saving the plot files. Defaults to an empty string.\n",
    "\n",
    "    Returns:\n",
    "        (tuple): A tuple of six arrays and one array of unique classes, where:\n",
    "            tp (np.ndarray): True positive counts for each class.\n",
    "            fp (np.ndarray): False positive counts for each class.\n",
    "            p (np.ndarray): Precision values at each confidence threshold.\n",
    "            r (np.ndarray): Recall values at each confidence threshold.\n",
    "            f1 (np.ndarray): F1-score values at each confidence threshold.\n",
    "            ap (np.ndarray): Average precision for each class at different IoU thresholds.\n",
    "            unique_classes (np.ndarray): An array of unique classes that have data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by objectness\n",
    "    i = np.argsort(-conf)\n",
    "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes, nt = np.unique(target_cls, return_counts=True)\n",
    "    nc = unique_classes.shape[0]  # number of classes, number of detections\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    px, py = np.linspace(0, 1, 1000), []  # for plotting\n",
    "    ap, p, r = np.zeros((nc, tp.shape[1])), np.zeros((nc, 1000)), np.zeros((nc, 1000))\n",
    "    for ci, c in enumerate(unique_classes):\n",
    "        i = pred_cls == c\n",
    "        n_l = nt[ci]  # number of labels\n",
    "        n_p = i.sum()  # number of predictions\n",
    "        if n_p == 0 or n_l == 0:\n",
    "            continue\n",
    "\n",
    "        # Accumulate FPs and TPs\n",
    "        fpc = (1 - tp[i]).cumsum(0)\n",
    "        tpc = tp[i].cumsum(0)\n",
    "\n",
    "        # Recall\n",
    "        recall = tpc / (n_l + eps)  # recall curve\n",
    "        r[ci] = np.interp(-px, -conf[i], recall[:, 0], left=0)  # negative x, xp because xp decreases\n",
    "\n",
    "        # Precision\n",
    "        precision = tpc / (tpc + fpc)  # precision curve\n",
    "        p[ci] = np.interp(-px, -conf[i], precision[:, 0], left=1)  # p at pr_score\n",
    "\n",
    "        # AP from recall-precision curve\n",
    "        for j in range(tp.shape[1]):\n",
    "            ap[ci, j], mpre, mrec = compute_ap(recall[:, j], precision[:, j])\n",
    "            if plot and j == 0:\n",
    "                py.append(np.interp(px, mrec, mpre))  # precision at mAP@0.5\n",
    "\n",
    "    # Compute F1 (harmonic mean of precision and recall)\n",
    "    f1 = 2 * p * r / (p + r + eps)\n",
    "    names = [v for k, v in names.items() if k in unique_classes]  # list: only classes that have data\n",
    "    names = dict(enumerate(names))  # to dict\n",
    "    if plot:\n",
    "        plot_pr_curve(px, py, ap, save_dir / f'{prefix}PR_curve.png', names, on_plot=on_plot)\n",
    "        plot_mc_curve(px, f1, save_dir / f'{prefix}F1_curve.png', names, ylabel='F1', on_plot=on_plot)\n",
    "        plot_mc_curve(px, p, save_dir / f'{prefix}P_curve.png', names, ylabel='Precision', on_plot=on_plot)\n",
    "        plot_mc_curve(px, r, save_dir / f'{prefix}R_curve.png', names, ylabel='Recall', on_plot=on_plot)\n",
    "\n",
    "    i = smooth(f1.mean(0), 0.1).argmax()  # max F1 index\n",
    "    p, r, f1 = p[:, i], r[:, i], f1[:, i]\n",
    "    tp = (r * nt).round()  # true positives\n",
    "    fp = (tp / (p + eps) - tp).round()  # false positives\n",
    "    return tp, fp, p, r, f1, ap, unique_classes.astype(int)\n",
    "\n",
    "\n",
    "class Metric(SimpleClass):\n",
    "    \"\"\"\n",
    "        Class for computing evaluation metrics for YOLOv8 model.\n",
    "\n",
    "        Attributes:\n",
    "            p (list): Precision for each class. Shape: (nc,).\n",
    "            r (list): Recall for each class. Shape: (nc,).\n",
    "            f1 (list): F1 score for each class. Shape: (nc,).\n",
    "            all_ap (list): AP scores for all classes and all IoU thresholds. Shape: (nc, 10).\n",
    "            ap_class_index (list): Index of class for each AP score. Shape: (nc,).\n",
    "            nc (int): Number of classes.\n",
    "\n",
    "        Methods:\n",
    "            ap50(): AP at IoU threshold of 0.5 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\n",
    "            ap(): AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\n",
    "            mp(): Mean precision of all classes. Returns: Float.\n",
    "            mr(): Mean recall of all classes. Returns: Float.\n",
    "            map50(): Mean AP at IoU threshold of 0.5 for all classes. Returns: Float.\n",
    "            map75(): Mean AP at IoU threshold of 0.75 for all classes. Returns: Float.\n",
    "            map(): Mean AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: Float.\n",
    "            mean_results(): Mean of results, returns mp, mr, map50, map.\n",
    "            class_result(i): Class-aware result, returns p[i], r[i], ap50[i], ap[i].\n",
    "            maps(): mAP of each class. Returns: Array of mAP scores, shape: (nc,).\n",
    "            fitness(): Model fitness as a weighted combination of metrics. Returns: Float.\n",
    "            update(results): Update metric attributes with new evaluation results.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.p = []  # (nc, )\n",
    "        self.r = []  # (nc, )\n",
    "        self.f1 = []  # (nc, )\n",
    "        self.all_ap = []  # (nc, 10)\n",
    "        self.ap_class_index = []  # (nc, )\n",
    "        self.nc = 0\n",
    "\n",
    "    @property\n",
    "    def ap50(self):\n",
    "        \"\"\"\n",
    "        Returns the Average Precision (AP) at an IoU threshold of 0.5 for all classes.\n",
    "\n",
    "        Returns:\n",
    "            (np.ndarray, list): Array of shape (nc,) with AP50 values per class, or an empty list if not available.\n",
    "        \"\"\"\n",
    "        return self.all_ap[:, 0] if len(self.all_ap) else []\n",
    "\n",
    "    @property\n",
    "    def ap(self):\n",
    "        \"\"\"\n",
    "        Returns the Average Precision (AP) at an IoU threshold of 0.5-0.95 for all classes.\n",
    "\n",
    "        Returns:\n",
    "            (np.ndarray, list): Array of shape (nc,) with AP50-95 values per class, or an empty list if not available.\n",
    "        \"\"\"\n",
    "        return self.all_ap.mean(1) if len(self.all_ap) else []\n",
    "\n",
    "    @property\n",
    "    def mp(self):\n",
    "        \"\"\"\n",
    "        Returns the Mean Precision of all classes.\n",
    "\n",
    "        Returns:\n",
    "            (float): The mean precision of all classes.\n",
    "        \"\"\"\n",
    "        return self.p.mean() if len(self.p) else 0.0\n",
    "\n",
    "    @property\n",
    "    def mr(self):\n",
    "        \"\"\"\n",
    "        Returns the Mean Recall of all classes.\n",
    "\n",
    "        Returns:\n",
    "            (float): The mean recall of all classes.\n",
    "        \"\"\"\n",
    "        return self.r.mean() if len(self.r) else 0.0\n",
    "\n",
    "    @property\n",
    "    def map50(self):\n",
    "        \"\"\"\n",
    "        Returns the mean Average Precision (mAP) at an IoU threshold of 0.5.\n",
    "\n",
    "        Returns:\n",
    "            (float): The mAP50 at an IoU threshold of 0.5.\n",
    "        \"\"\"\n",
    "        return self.all_ap[:, 0].mean() if len(self.all_ap) else 0.0\n",
    "\n",
    "    @property\n",
    "    def map75(self):\n",
    "        \"\"\"\n",
    "        Returns the mean Average Precision (mAP) at an IoU threshold of 0.75.\n",
    "\n",
    "        Returns:\n",
    "            (float): The mAP50 at an IoU threshold of 0.75.\n",
    "        \"\"\"\n",
    "        return self.all_ap[:, 5].mean() if len(self.all_ap) else 0.0\n",
    "\n",
    "    @property\n",
    "    def map(self):\n",
    "        \"\"\"\n",
    "        Returns the mean Average Precision (mAP) over IoU thresholds of 0.5 - 0.95 in steps of 0.05.\n",
    "\n",
    "        Returns:\n",
    "            (float): The mAP over IoU thresholds of 0.5 - 0.95 in steps of 0.05.\n",
    "        \"\"\"\n",
    "        return self.all_ap.mean() if len(self.all_ap) else 0.0\n",
    "\n",
    "    def mean_results(self):\n",
    "        \"\"\"Mean of results, return mp, mr, map50, map.\"\"\"\n",
    "        return [self.mp, self.mr, self.map50, self.map]\n",
    "\n",
    "    def class_result(self, i):\n",
    "        \"\"\"class-aware result, return p[i], r[i], ap50[i], ap[i].\"\"\"\n",
    "        return self.p[i], self.r[i], self.ap50[i], self.ap[i]\n",
    "\n",
    "    @property\n",
    "    def maps(self):\n",
    "        \"\"\"mAP of each class.\"\"\"\n",
    "        maps = np.zeros(self.nc) + self.map\n",
    "        for i, c in enumerate(self.ap_class_index):\n",
    "            maps[c] = self.ap[i]\n",
    "        return maps\n",
    "\n",
    "    def fitness(self):\n",
    "        \"\"\"Model fitness as a weighted combination of metrics.\"\"\"\n",
    "        w = [0.0, 0.0, 0.1, 0.9]  # weights for [P, R, mAP@0.5, mAP@0.5:0.95]\n",
    "        return (np.array(self.mean_results()) * w).sum()\n",
    "\n",
    "    def update(self, results):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            results (tuple): A tuple of (p, r, ap, f1, ap_class)\n",
    "        \"\"\"\n",
    "        self.p, self.r, self.f1, self.all_ap, self.ap_class_index = results\n",
    "\n",
    "\n",
    "class DetMetrics(SimpleClass):\n",
    "    \"\"\"\n",
    "    This class is a utility class for computing detection metrics such as precision, recall, and mean average precision\n",
    "    (mAP) of an object detection model.\n",
    "\n",
    "    Args:\n",
    "        save_dir (Path): A path to the directory where the output plots will be saved. Defaults to current directory.\n",
    "        plot (bool): A flag that indicates whether to plot precision-recall curves for each class. Defaults to False.\n",
    "        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n",
    "        names (tuple of str): A tuple of strings that represents the names of the classes. Defaults to an empty tuple.\n",
    "\n",
    "    Attributes:\n",
    "        save_dir (Path): A path to the directory where the output plots will be saved.\n",
    "        plot (bool): A flag that indicates whether to plot the precision-recall curves for each class.\n",
    "        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n",
    "        names (tuple of str): A tuple of strings that represents the names of the classes.\n",
    "        box (Metric): An instance of the Metric class for storing the results of the detection metrics.\n",
    "        speed (dict): A dictionary for storing the execution time of different parts of the detection process.\n",
    "\n",
    "    Methods:\n",
    "        process(tp, conf, pred_cls, target_cls): Updates the metric results with the latest batch of predictions.\n",
    "        keys: Returns a list of keys for accessing the computed detection metrics.\n",
    "        mean_results: Returns a list of mean values for the computed detection metrics.\n",
    "        class_result(i): Returns a list of values for the computed detection metrics for a specific class.\n",
    "        maps: Returns a dictionary of mean average precision (mAP) values for different IoU thresholds.\n",
    "        fitness: Computes the fitness score based on the computed detection metrics.\n",
    "        ap_class_index: Returns a list of class indices sorted by their average precision (AP) values.\n",
    "        results_dict: Returns a dictionary that maps detection metric keys to their computed values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, save_dir=Path('.'), plot=False, on_plot=None, names=()) -> None:\n",
    "        self.save_dir = save_dir\n",
    "        self.plot = plot\n",
    "        self.on_plot = on_plot\n",
    "        self.names = names\n",
    "        self.box = Metric()\n",
    "        self.speed = {'preprocess': 0.0, 'inference': 0.0, 'loss': 0.0, 'postprocess': 0.0}\n",
    "\n",
    "    def process(self, tp, conf, pred_cls, target_cls):\n",
    "        \"\"\"Process predicted results for object detection and update metrics.\"\"\"\n",
    "        results = ap_per_class(tp,\n",
    "                               conf,\n",
    "                               pred_cls,\n",
    "                               target_cls,\n",
    "                               plot=self.plot,\n",
    "                               save_dir=self.save_dir,\n",
    "                               names=self.names,\n",
    "                               on_plot=self.on_plot)[2:]\n",
    "        self.box.nc = len(self.names)\n",
    "        self.box.update(results)\n",
    "\n",
    "    @property\n",
    "    def keys(self):\n",
    "        \"\"\"Returns a list of keys for accessing specific metrics.\"\"\"\n",
    "        return ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
    "\n",
    "    def mean_results(self):\n",
    "        \"\"\"Calculate mean of detected objects & return precision, recall, mAP50, and mAP50-95.\"\"\"\n",
    "        return self.box.mean_results()\n",
    "\n",
    "    def class_result(self, i):\n",
    "        \"\"\"Return the result of evaluating the performance of an object detection model on a specific class.\"\"\"\n",
    "        return self.box.class_result(i)\n",
    "\n",
    "    @property\n",
    "    def maps(self):\n",
    "        \"\"\"Returns mean Average Precision (mAP) scores per class.\"\"\"\n",
    "        return self.box.maps\n",
    "\n",
    "    @property\n",
    "    def fitness(self):\n",
    "        \"\"\"Returns the fitness of box object.\"\"\"\n",
    "        return self.box.fitness()\n",
    "\n",
    "    @property\n",
    "    def ap_class_index(self):\n",
    "        \"\"\"Returns the average precision index per class.\"\"\"\n",
    "        return self.box.ap_class_index\n",
    "\n",
    "    @property\n",
    "    def results_dict(self):\n",
    "        \"\"\"Returns dictionary of computed performance metrics and statistics.\"\"\"\n",
    "        return dict(zip(self.keys + ['fitness'], self.mean_results() + [self.fitness]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(\n",
    "        prediction,\n",
    "        conf_thres=0.25,\n",
    "        iou_thres=0.45,\n",
    "        classes=None,\n",
    "        agnostic=False,\n",
    "        multi_label=False,\n",
    "        labels=(),\n",
    "        max_det=300,\n",
    "        nc=0,  # number of classes (optional)\n",
    "        max_time_img=0.05,\n",
    "        max_nms=30000,\n",
    "        max_wh=7680,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform non-maximum suppression (NMS) on a set of boxes, with support for masks and multiple labels per box.\n",
    "\n",
    "    Arguments:\n",
    "        prediction (torch.Tensor): A tensor of shape (batch_size, num_classes + 4 + num_masks, num_boxes)\n",
    "            containing the predicted boxes, classes, and masks. The tensor should be in the format\n",
    "            output by a model, such as YOLO.\n",
    "        conf_thres (float): The confidence threshold below which boxes will be filtered out.\n",
    "            Valid values are between 0.0 and 1.0.\n",
    "        iou_thres (float): The IoU threshold below which boxes will be filtered out during NMS.\n",
    "            Valid values are between 0.0 and 1.0.\n",
    "        classes (List[int]): A list of class indices to consider. If None, all classes will be considered.\n",
    "        agnostic (bool): If True, the model is agnostic to the number of classes, and all\n",
    "            classes will be considered as one.\n",
    "        multi_label (bool): If True, each box may have multiple labels.\n",
    "        labels (List[List[Union[int, float, torch.Tensor]]]): A list of lists, where each inner\n",
    "            list contains the apriori labels for a given image. The list should be in the format\n",
    "            output by a dataloader, with each label being a tuple of (class_index, x1, y1, x2, y2).\n",
    "        max_det (int): The maximum number of boxes to keep after NMS.\n",
    "        nc (int): (optional) The number of classes output by the model. Any indices after this will be considered masks.\n",
    "        max_time_img (float): The maximum time (seconds) for processing one image.\n",
    "        max_nms (int): The maximum number of boxes into torchvision.ops.nms().\n",
    "        max_wh (int): The maximum box width and height in pixels\n",
    "\n",
    "    Returns:\n",
    "        (List[torch.Tensor]): A list of length batch_size, where each element is a tensor of\n",
    "            shape (num_boxes, 6 + num_masks) containing the kept boxes, with columns\n",
    "            (x1, y1, x2, y2, confidence, class, mask1, mask2, ...).\n",
    "    \"\"\"\n",
    "\n",
    "    # Checks\n",
    "    assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
    "    assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
    "    if isinstance(prediction, (list, tuple)):  # YOLOv8 model in validation model, output = (inference_out, loss_out)\n",
    "        prediction = prediction[0]  # select only inference output\n",
    "\n",
    "    device = prediction.device\n",
    "    mps = 'mps' in device.type  # Apple MPS\n",
    "    if mps:  # MPS not fully supported yet, convert tensors to CPU before NMS\n",
    "        prediction = prediction.cpu()\n",
    "    bs = prediction.shape[0]  # batch size\n",
    "    nc = nc or (prediction.shape[1] - 4)  # number of classes\n",
    "    nm = prediction.shape[1] - nc - 4\n",
    "    mi = 4 + nc  # mask start index\n",
    "    xc = prediction[:, 4:mi].amax(1) > conf_thres  # candidates\n",
    "\n",
    "    # Settings\n",
    "    # min_wh = 2  # (pixels) minimum box width and height\n",
    "    time_limit = 0.5 + max_time_img * bs  # seconds to quit after\n",
    "    redundant = True  # require redundant detections\n",
    "    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "    merge = False  # use merge-NMS\n",
    "\n",
    "    t = time.time()\n",
    "    output = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[:, 2:4] < min_wh) | (x[:, 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        x = x.transpose(0, -1)[xc[xi]]  # confidence\n",
    "\n",
    "        # Cat apriori labels if autolabelling\n",
    "        if labels and len(labels[xi]):\n",
    "            lb = labels[xi]\n",
    "            v = torch.zeros((len(lb), nc + nm + 5), device=x.device)\n",
    "            v[:, :4] = lb[:, 1:5]  # box\n",
    "            v[range(len(lb)), lb[:, 0].long() + 4] = 1.0  # cls\n",
    "            x = torch.cat((x, v), 0)\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        box, cls, mask = x.split((4, nc, nm), 1)\n",
    "        box = xywh2xyxy(box)  # center_x, center_y, width, height) to (x1, y1, x2, y2)\n",
    "        if multi_label:\n",
    "            i, j = (cls > conf_thres).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float(), mask[i]), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = cls.max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        # Filter by class\n",
    "        if classes is not None:\n",
    "            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
    "\n",
    "        # Apply finite constraint\n",
    "        # if not torch.isfinite(x).all():\n",
    "        #     x = x[torch.isfinite(x).all(1)]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence and remove excess boxes\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        i = i[:max_det]  # limit detections\n",
    "        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
    "            # Update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
    "            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
    "            weights = iou * scores[None]  # box weights\n",
    "            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
    "            if redundant:\n",
    "                i = i[iou.sum(1) > 1]  # require redundancy\n",
    "\n",
    "        output[xi] = x[i]\n",
    "        if mps:\n",
    "            output[xi] = output[xi].to(device)\n",
    "        if (time.time() - t) > time_limit:\n",
    "            LOGGER.warning(f'WARNING ⚠️ NMS time limit {time_limit:.3f}s exceeded')\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_batch(self, detections, labels):\n",
    "    \"\"\"\n",
    "    Return correct prediction matrix\n",
    "    Arguments:\n",
    "        detections (array[N, 6]), x1, y1, x2, y2, conf, class\n",
    "        labels (array[M, 5]), class, x1, y1, x2, y2\n",
    "    Returns:\n",
    "        correct (array[N, 10]), for 10 IoU levels\n",
    "    \"\"\"\n",
    "    iou = box_iou(labels[:, 1:], detections[:, :4])\n",
    "    correct = np.zeros((detections.shape[0], self.iouv.shape[0])).astype(bool)\n",
    "    correct_class = labels[:, 0:1] == detections[:, 5]\n",
    "    for i in range(len(self.iouv)):\n",
    "        x = torch.where((iou >= self.iouv[i]) & correct_class)  # IoU > threshold and classes match\n",
    "        if x[0].shape[0]:\n",
    "            matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()  # [label, detect, iou]\n",
    "            if x[0].shape[0] > 1:\n",
    "                matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "                matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n",
    "                # matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "                matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n",
    "            correct[matches[:, 1].astype(int), i] = True\n",
    "    return torch.tensor(correct, dtype=torch.bool, device=detections.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = dict(type='ImageClassifier',\n",
    "            data_preprocessor=dict(type='DataPreprocessor'),\n",
    "            backbone=dict(type='ResNet', in_channels=3, out_channels = 3),\n",
    "            neck=dict(type='GlobalAveragePooling'),\n",
    "            head=dict(type='LinearClsHead',\n",
    "                      loss=dict(type='CrossEntropyLoss'), \n",
    "                      num_classes=100, \n",
    "                      in_channels = 3),)\n",
    "\n",
    "train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='RandomResizedCrop', scale = 224),\n",
    "    dict(type='PackInputs'),\n",
    "]\n",
    "\n",
    "train_dataloader = dict(\n",
    "    batch_size=8,\n",
    "    num_workers=5,\n",
    "    dataset=dict(type='CustomDataset', data_root='images/train/', pipeline=train_pipeline),\n",
    ")\n",
    "\n",
    "optim_wrapper = dict(type='OptimWrapper', optimizer=dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001))\n",
    "\n",
    "\n",
    "data_preprocessor = dict(\n",
    "    num_classes=10,\n",
    "    # RGB format normalization parameters\n",
    "    mean=[123.675, 116.28, 103.53],\n",
    "    std=[58.395, 57.12, 57.375],\n",
    "    # convert image from BGR to RGB\n",
    "    to_rgb=True,\n",
    ")\n",
    "\n",
    "hook = dict(type='Hook', h=66666666)\n",
    "\n",
    "train_cfg = dict(type='Loop', \n",
    "                 max_epochs=10, \n",
    "                 val_begin=50, \n",
    "                 val_interval=20)\n",
    "\n",
    "cfg = dict(type='Runner',model=model,\n",
    "                optim_wrapper=optim_wrapper,\n",
    "                custom_hooks=hook,\n",
    "                train_dataloader=train_dataloader,\n",
    "                train_cfg=train_cfg,\n",
    "                data_preprocessor=data_preprocessor)\n",
    "\n",
    "runner = RUNNERS.build(cfg)\n",
    "\n",
    "\n",
    "runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
